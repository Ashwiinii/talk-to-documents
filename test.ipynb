{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"./Ashwini Muralidharan Cover Letter.pdf\"\n",
    "document_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the Hiring Committee, I am writing to express my interest in the Scientist-Data and Modeler position at First Solar . With a Master's degree in Electrical Engineering from North Carolina State University and extensive experience in data analysis, machine learning, and statistical modeling, I am excited about the opportunity to contribute to your team and support organizational strategy and operations through data-driven insights. In my role as a Data Science Intern at Native Nibbles, I conduc\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = clean_text(document_text)\n",
    "print(cleaned_text[:500])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To the Hiring Committee, I am writing to express my interest in the Scientist-Data and Modeler position at First Solar . With a Master's degree in Electrical Engineering from North Carolina State University and extensive experience in data analysis, machine learning, and statistical modeling, I am excited about the opportunity to contribute to your team and support organizational strategy and operations through data-driven insights. In my role as a Data Science Intern at Native Nibbles, I\", 'as a Data Science Intern at Native Nibbles, I conducted comprehensive data extraction, cleaning, and transformation, which aligns well with the responsibilities of extracting and transforming data from existing relational database management systems. My experience with SQL and relational databases, along with my proficiency in Python and data visualization tools like Tableau, has equipped me to perform exploratory analysis and support project needs effectively . I conducted comprehensive data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(cleaned_text)\n",
    "print(chunks[:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To the Hiring Committee, I am writing to express my interest in the Scientist-Data and Modeler position at First Solar . With a Master's degree in Electrical Engineering from North Carolina State University and extensive experience in data analysis, machine learning, and statistical modeling, I am excited about the opportunity to contribute to your team and support organizational strategy and operations through data-driven insights. In my role as a Data Science Intern at Native Nibbles, I\", 'as a Data Science Intern at Native Nibbles, I conducted comprehensive data extraction, cleaning, and transformation, which aligns well with the responsibilities of extracting and transforming data from existing relational database management systems. My experience with SQL and relational databases, along with my proficiency in Python and data visualization tools like Tableau, has equipped me to perform exploratory analysis and support project needs effectively . I conducted comprehensive data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(cleaned_text)\n",
    "print(chunks[:2])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d3e1147f9a4055be0bdd62e0f6c466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ashwi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bbd682b00048eb83b542362574b647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f8a4ecfaf451eb1f5f99fb6c37e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38294812a0884abfacd1b00fc5b88572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb463b1a3704570a28eba0609856687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ddcd2e0cc84396a47c6055db47d1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ace8e9483e94489995daab32672f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7177ab48f99409f8272c11b7f7fd627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b248814c4ce4610a166b90282fbf9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3671991df41c47f4b080413582cc7564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c84e06432c4fe0af08e4dce59589ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = generate_embeddings(chunks)\n",
    "print(embeddings.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]  \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "index = create_faiss_index(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching chunks:\n",
      "role as a Deep Learning Engineer at the Vázquez Research Group, I developed and integrated complex data pipelines, applied advanced machine learning techniques, and communicated findings effectively to stakeholders. My experience in Python, SQL, and data visualization tools like PowerBI and Tableau aligns well with the technical requirements of this position. Additionally , I have worked extensively with data science and machine learning packages such as Numpy , Pandas, Scikit-learn, TensorFlow\n",
      "well with your requirement for proficiency in querying databases and visualizing insights effectively . This role required a deep understanding of statistical methods and the ability to apply them to real-world business problems, similar to the responsibilities at First Solar . My experience in designing and conducting experiments, analyzing data, and presenting actionable insights to management has prepared me well for this role. In my previous role as a Deep Learning Engineer at the Vázquez\n",
      "my skills to build and deploy machine learning models, conduct advanced statistical analysis, and contribute to process improvement initiatives. I am confident that my background in data science, combined with my strong communication and presentation skills, will allow me to make a meaningful impact on your team. Thank you for considering my application. I look forward to the opportunity to discuss how my background and skills align with the needs of First Solar . Thanks and regards, Ashwini\n",
      "To the Hiring Committee, I am writing to express my interest in the Scientist-Data and Modeler position at First Solar . With a Master's degree in Electrical Engineering from North Carolina State University and extensive experience in data analysis, machine learning, and statistical modeling, I am excited about the opportunity to contribute to your team and support organizational strategy and operations through data-driven insights. In my role as a Data Science Intern at Native Nibbles, I\n",
      "model accuracy and processing speed through advanced clustering and optimization techniques, resulting in significant business impact.● Statistical Analysis and Model Development: Conducted comprehensive data analysis and built predictive models to support business objectives, leveraging techniques such as regression, clustering, and time series analysis. I am particularly excited about the opportunity to work at First Solar , where I can leverage my skills to build and deploy machine learning\n"
     ]
    }
   ],
   "source": [
    "def search_index(query, model, index, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "query = \"experience\"\n",
    "distances, indices = search_index(query, model, index)\n",
    "\n",
    "top_chunks = [chunks[i] for i in indices[0]]\n",
    "print(\"Top matching chunks:\")\n",
    "for chunk in top_chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\ashwi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c54528f471406f843d5e650234d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ashwi\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3679fa75c446dba3e02af88b14bbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a053f61a42745f485cfea90dab92b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a322b741f34ae0a9cade3c6a0ab55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c1aff96d7948bcbc81c311d29eb371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(query, model, tokenizer, index, chunks, top_k=3):\n",
    "    distances, indices = search_index(query, model, index, top_k)\n",
    "    \n",
    "    context = \"\\n\".join([chunks[i] for i in indices[0]])\n",
    "    \n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=2048, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=inputs[\"input_ids\"], max_length=512, num_return_sequences=1, temperature=0.7)\n",
    "    \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "query = \"What is the summary of the document?\"\n",
    "answer = generate_answer(query, model, tokenizer, index, chunks)\n",
    "print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
